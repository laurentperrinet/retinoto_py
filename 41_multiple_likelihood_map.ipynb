{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ce09631-1c66-4af0-b7fa-08e78d32a471",
   "metadata": {},
   "source": [
    "Building likelihood maps for a set of representative images.\n",
    "\n",
    "> TODO: use images from dataset and superimpose the bounding boxes on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8130a92-df14-4988-a525-d2f2d9f68f12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T09:35:20.433866Z",
     "iopub.status.busy": "2025-04-26T09:35:20.433584Z",
     "iopub.status.idle": "2025-04-26T09:35:32.469394Z",
     "shell.execute_reply": "2025-04-26T09:35:32.468897Z"
    }
   },
   "outputs": [],
   "source": [
    "import retinoto_py as fovea\n",
    "args = fovea.Params(do_fovea=True, batch_size=1)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13479bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6e1fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_to_label = fovea.get_idx_to_label(args)\n",
    "label2idx = fovea.get_label_to_idx(args)\n",
    "label2idx['impala']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eef4fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in label2idx.keys(): \n",
    "    if 'wolf' in key.lower(): print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4079936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'bbox'\n",
    "model_filename = args.data_cache / f'32_fovea_model_name={args.model_name}_dataset={dataset}.pth'\n",
    "model = fovea.load_model(args, model_filename=model_filename)\n",
    "model_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c223451-98af-4ed1-ab56-838b4deaf5b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T09:35:32.471565Z",
     "iopub.status.busy": "2025-04-26T09:35:32.471195Z",
     "iopub.status.idle": "2025-04-26T09:35:33.438316Z",
     "shell.execute_reply": "2025-04-26T09:35:33.437592Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import InterpolationMode, resize\n",
    "image_size_full = 512\n",
    "resolution = (54, 34)\n",
    "resolution = (34, 21)\n",
    "resolution = (21, 13)\n",
    "size_ratio = .4\n",
    "alpha = .8\n",
    "s_max = 200\n",
    "import cmocean\n",
    "cmap = cmocean.cm.haline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1613063",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_images = [('leopard.jpg',  'leopard'),\n",
    "                # ('wolf.jpg',  'white_wolf'),\n",
    "                ('frog.jpg',  'tree_frog'),\n",
    "                # ('Hidden_snow_leopard.png',  'snow_leopard'),\n",
    "                ('Hidden_leopard.png',  'leopard'),\n",
    "                # ('Hidden_giraffe.png',  'girafe'),\n",
    "                ('Hidden_tiger.png',  'tiger'),\n",
    "                ('Hidden_owl.png',  'great_grey_owl'),\n",
    "                ]\n",
    "\n",
    "# list_images = []\n",
    "\n",
    "for image_url, true_label in  list_images:\n",
    "    full_image = read_image('./images/' + image_url)[:3, :, :]/255\n",
    "    full_image = resize(full_image, image_size_full, interpolation=InterpolationMode.BILINEAR, antialias=True)\n",
    "    # print(f\"{type(full_image) = }, {full_image.dtype = }, {full_image.shape = }\")\n",
    "\n",
    "    for do_fovea  in [True, False]:\n",
    "        print('do_fovea = ', do_fovea)\n",
    "        args = fovea.Params(do_mask=not(do_fovea), do_fovea=do_fovea)\n",
    "        pos_H, pos_W = fovea.get_positions(full_image.shape[1], full_image.shape[2], resolution=resolution)\n",
    "        probas = fovea.compute_likelihood_map(args, model, full_image, size_ratio=size_ratio)\n",
    "        probas = probas.cpu()\n",
    "        proba_label = probas[:, label2idx[true_label]]\n",
    "        idx_max = proba_label.argmax()\n",
    "\n",
    "\n",
    "        fig, ax = fovea.plt.subplots()\n",
    "        full_image_np = torch.movedim(full_image, (1, 2, 0), (0, 1, 2)).numpy()\n",
    "        ax.imshow(full_image_np)\n",
    "        scatter = ax.scatter(pos_W, pos_H, s=proba_label.abs()*s_max, c=proba_label, alpha=alpha, edgecolors='none', cmap=cmap, vmin=0, vmax=1)\n",
    "        print(f'Max proba_label ={max(proba_label).item():.3f}')\n",
    "\n",
    "        ax.scatter(pos_W[idx_max], pos_H[idx_max], s=proba_label[idx_max]*s_max, marker='*', c='red', alpha=alpha)\n",
    "        fig.colorbar(scatter, ax=ax)  # Add colorbar\n",
    "\n",
    "        fig.set_facecolor(color='white')\n",
    "        fovea.plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a360588",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for image_url, true_label in list_images:\n",
    "    full_image = read_image('./images/' + image_url)[:3, :, :]/255\n",
    "    full_image = resize(full_image, image_size_full, interpolation=InterpolationMode.BILINEAR, antialias=True)\n",
    "    # print(f\"{type(full_image) = }, {full_image.dtype = }, {full_image.shape = }\")\n",
    "\n",
    "    for do_fovea  in [True, False]:\n",
    "        args = fovea.Params(do_mask=not(do_fovea), do_fovea=do_fovea)\n",
    "\n",
    "        pos_H, pos_W = fovea.get_positions(full_image.shape[1], full_image.shape[2], resolution=resolution)\n",
    "        probas = fovea.compute_likelihood_map(args, model, full_image, size_ratio=size_ratio)\n",
    "\n",
    "        probas = probas.cpu()\n",
    "        proba_label = probas[:, label2idx[true_label]]\n",
    "        idx_max = proba_label.argmax()\n",
    "\n",
    "        print(50*'=-')\n",
    "        print(f'{image_url=} contains a {true_label}?')\n",
    "        print(50*'.-')\n",
    "        fig, ax = fovea.plt.subplots()\n",
    "        full_image_np = torch.movedim(full_image, (1, 2, 0), (0, 1, 2)).numpy()\n",
    "        ax.imshow(full_image_np)\n",
    "        scatter = ax.scatter(pos_W, pos_H, s=proba_label.abs()*s_max, c=proba_label, alpha=alpha, edgecolors='none', cmap=cmap, vmin=0, vmax=1)\n",
    "        print(f'Max proba_label ={max(proba_label).item():.3f}')\n",
    "\n",
    "        ax.scatter(pos_W[idx_max], pos_H[idx_max], s=proba_label[idx_max]*s_max, marker='*', c='red', alpha=alpha)\n",
    "        fig.colorbar(scatter, ax=ax)  # Add colorbar\n",
    "\n",
    "        fig.set_facecolor(color='white')\n",
    "        fovea.plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba96d6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e0d556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2 as transforms\n",
    "dataset = 'full'\n",
    "VAL_DATA_DIR = args.DATAROOT / f'Imagenet_{dataset}' / 'val'\n",
    "val_dataset = fovea.get_dataset(args, VAL_DATA_DIR, do_full_preprocess=False)\n",
    "val_loader = fovea.get_loader(args, val_dataset)\n",
    "\n",
    "for i_batch, (image, true_idx) in enumerate(val_loader):\n",
    "    if i_batch >= n_batch : break\n",
    "    # Since batch_size=1, the current index is just i_batch\n",
    "    # Get the path of the current image\n",
    "    path = val_loader.dataset.imgs[i_batch][0]\n",
    "    print(\"Image path:\", path)\n",
    "    image, true_idx = image.to(args.device), true_idx.to(args.device)\n",
    "    image = image.squeeze(0)    \n",
    "\n",
    "    crop_size = max(image.shape[-2], image.shape[-1])\n",
    "    pad_width = max(0, (crop_size - image.shape[-1]) // 2)\n",
    "    pad_height = max(0, (crop_size - image.shape[-1]) // 2)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Pad((pad_width, pad_width, pad_height, pad_height), padding_mode='reflect'),\n",
    "        transforms.CenterCrop(crop_size),\n",
    "    ])\n",
    "    image = transform(image).squeeze(0)   \n",
    "        \n",
    "    fig, ax = fovea.plt.subplots()\n",
    "    image_np = torch.movedim(image, (1, 2, 0), (0, 1, 2)).cpu().numpy()\n",
    "    ax.imshow(image_np)\n",
    "    fig.set_facecolor(color='white')\n",
    "    fovea.plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f69be65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.transforms import v2 as transforms\n",
    "# args = fovea.Params(do_fovea=True, batch_size=1)\n",
    "# for dataset in fovea.params.all_datasets:\n",
    "#     VAL_DATA_DIR = args.DATAROOT / f'Imagenet_{dataset}' / 'val'\n",
    "#     val_dataset = fovea.get_dataset(args, VAL_DATA_DIR, do_full_preprocess=False)\n",
    "#     val_loader = fovea.get_loader(args, val_dataset)\n",
    "    \n",
    "#     print('dataset=', dataset, ' from', VAL_DATA_DIR, 'dataset=', dataset)\n",
    "#     print('dataset=', VAL_DATA_DIR)\n",
    "\n",
    "#     for i_batch, (image, true_idx) in fovea.tqdm(enumerate(val_loader), total=n_batch):\n",
    "#         print(50*'=-')\n",
    "#         # print(f'{image_url=} contains a {true_label}?')\n",
    "#         print(f'image contains a {true_idx} = {val_loader.dataset.idx2label[true_idx]}')\n",
    "#         print(50*'.-')\n",
    "\n",
    "#         if i_batch >= n_batch : break\n",
    "#         image, true_idx = image.to(args.device), true_idx.to(args.device)\n",
    "#         image = image.squeeze(0)\n",
    "#         crop_size = min(image.shape[1], image.shape[2])\n",
    "#         image = transforms.CenterCrop(crop_size)(image)\n",
    "\n",
    "#         pos_H, pos_W, probas = fovea.compute_likelihood_map(args, model, image.squeeze(0), size_ratio=size_ratio, resolution=resolution)\n",
    "#         probas = probas.cpu()\n",
    "#         proba_label = probas[:, true_idx.cpu()]\n",
    "#         idx_max = proba_label.argmax()\n",
    "\n",
    "\n",
    "\n",
    "#         fig, ax = fovea.plt.subplots()\n",
    "#         image_np = torch.movedim(image, (1, 2, 0), (0, 1, 2)).cpu().numpy()\n",
    "#         ax.imshow(image_np)\n",
    "#         scatter = ax.scatter(pos_W, pos_H, s=proba_label.abs()*s_max, c=proba_label, alpha=alpha, edgecolors='none', cmap=cmap, vmin=0, vmax=1)\n",
    "#         print(f'Max proba_label ={max(proba_label).item():.3f}')\n",
    "\n",
    "#         ax.scatter(pos_W[idx_max], pos_H[idx_max], s=proba_label[idx_max]*s_max, marker='*', c='red', alpha=alpha)\n",
    "#         fig.colorbar(scatter, ax=ax)  # Add colorbar\n",
    "\n",
    "#         fig.set_facecolor(color='white')\n",
    "#         fovea.plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267ba19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2 as transforms\n",
    "# from torchvision import transforms\n",
    "args = fovea.Params(do_fovea=True, batch_size=1)\n",
    "for dataset in fovea.params.all_datasets:\n",
    "    VAL_DATA_DIR = args.DATAROOT / f'Imagenet_{dataset}' / 'val'\n",
    "    val_dataset = fovea.get_dataset(args, VAL_DATA_DIR, do_full_preprocess=False)\n",
    "    val_loader = fovea.get_loader(args, val_dataset)\n",
    "\n",
    "    for i_batch, (image, true_idx) in fovea.tqdm(enumerate(val_loader), total=n_batch):\n",
    "        if i_batch >= n_batch : break\n",
    "\n",
    "        print(50*'=-')\n",
    "        # print(f'{image_url=} contains a {true_label}?')\n",
    "        print(f'image contains a {true_idx} = {val_loader.dataset.idx2label[true_idx]}')\n",
    "        print(50*'.-')\n",
    "\n",
    "        image, true_idx = image.to(args.device), true_idx.to(args.device)\n",
    "        \n",
    "        crop_size = max(image.shape[-2], image.shape[-1])\n",
    "        pad_width = max(0, (crop_size - image.shape[-1]) // 2)\n",
    "        pad_height = max(0, (crop_size - image.shape[-1]) // 2)\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Pad((pad_width, pad_width, pad_height, pad_height), padding_mode='reflect'),\n",
    "            transforms.CenterCrop(crop_size),\n",
    "        ])\n",
    "        image = transform(image).squeeze(0)   \n",
    "\n",
    "        pos_H, pos_W = fovea.get_positions(image.shape[1], image.shape[2], resolution=resolution)\n",
    "        probas = fovea.compute_likelihood_map(args, model, full_image, size_ratio=size_ratio)\n",
    "        probas = probas.cpu()\n",
    "        proba_label = probas[:, true_idx.cpu()]\n",
    "        idx_max = proba_label.argmax()\n",
    "\n",
    "        fig, ax = fovea.plt.subplots()\n",
    "        image_np = torch.movedim(image, (1, 2, 0), (0, 1, 2)).cpu().numpy()\n",
    "        ax.imshow(image_np)\n",
    "        scatter = ax.scatter(pos_W, pos_H, s=proba_label.abs()*s_max, c=proba_label, alpha=alpha, edgecolors='none', cmap=cmap, vmin=0, vmax=1)\n",
    "        print(f'Max proba_label ={max(proba_label).item():.3f}')\n",
    "\n",
    "        ax.scatter(pos_W[idx_max], pos_H[idx_max], s=proba_label[idx_max]*s_max, marker='*', c='red', alpha=alpha)\n",
    "        fig.colorbar(scatter, ax=ax)  # Add colorbar\n",
    "\n",
    "        fig.set_facecolor(color='white')\n",
    "        fovea.plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d5b820",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
